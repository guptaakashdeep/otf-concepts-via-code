{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/akashdeepgupta/Documents/project-repos/pyspark-35-setup/spark-3.5.3-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "DW_PATH='/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse'\n",
    "# SPARK_VERSION='3.5'\n",
    "# ICEBERG_VERSION='1.5.0'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"iceberg-poc\") \\\n",
    "    .config('spark.jars.packages', f'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.spark:spark-avro_2.12:3.5.0')\\\n",
    "    .config('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\\\n",
    "    .config('spark.sql.catalog.local','org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.local.type','hadoop') \\\n",
    "    .config('spark.sql.catalog.local.warehouse',DW_PATH) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, lit, array, rand, when, col\n",
    "\n",
    "TGT_TBL = \"local.db.emp_bv_details\"\n",
    "\n",
    "t1 = spark.range(30000).withColumn(\"year\", \n",
    "                                when(col(\"id\") <= 10000, lit(2023))\\\n",
    "                                .when(col(\"id\").between(10001, 15000), lit(2024))\\\n",
    "                                .otherwise(lit(2025))\n",
    "                                )\n",
    "t1 = t1.withColumn(\"business_vertical\", array(\n",
    "        lit(\"Retail\"), \n",
    "        lit(\"SME\"), \n",
    "        lit(\"Cor\"), \n",
    "        lit(\"Analytics\")\n",
    "        ).getItem((rand()*4).cast(\"int\")))\\\n",
    "        .withColumn(\"is_updated\", lit(False))\n",
    "\n",
    "t1.coalesce(1).writeTo(TGT_TBL).partitionedBy('year').using('iceberg')\\\n",
    "    .tableProperty('format-version','2')\\\n",
    "    .tableProperty('write.delete.mode','merge-on-read')\\\n",
    "    .tableProperty('write.update.mode','merge-on-read')\\\n",
    "    .tableProperty('write.merge.mode','merge-on-read')\\\n",
    "    .create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+\n",
      "|year|min_id|max_id|\n",
      "+----+------+------+\n",
      "|2023|     0| 10000|\n",
      "|2024| 10001| 15000|\n",
      "|2025| 15001| 29999|\n",
      "+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min\n",
    "spark.table(TGT_TBL).groupBy(\"year\").agg(min(\"id\").alias(\"min_id\"), max(\"id\").alias(\"max_id\")).orderBy(\"year\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New department created called Sales and 3000 employees switched in 2025\n",
    "updated_records = spark.range(15000, 18001).withColumn(\"year\", lit(2025)).withColumn(\"business_vertical\", lit(\"Sales\"))\n",
    "STG_TBL = \"local.db.emp_bv_updates\"\n",
    "\n",
    "# updated_records.groupBy(\"year\").agg(min(\"id\")).show()\n",
    "updated_records.coalesce(1).writeTo(STG_TBL).partitionedBy('year').using('iceberg')\\\n",
    "    .tableProperty('format-version','2')\\\n",
    "    .create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {TGT_TBL} as tgt\n",
    "USING (Select *, False as is_updated from {STG_TBL}) as src\n",
    "ON tgt.id = src.id\n",
    "WHEN MATCHED AND src.business_vertical <> tgt.business_vertical AND tgt.year = src.year THEN\n",
    "    UPDATE SET tgt.is_updated = True, tgt.business_vertical = src.business_vertical\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Plan BEFORE Optimization\n",
    "\n",
    "```sql\n",
    "== Physical Plan ==\n",
    "WriteDelta (32)\n",
    "+- AdaptiveSparkPlan (31)\n",
    "   +- == Final Plan ==\n",
    "      AQEShuffleRead (20)\n",
    "      +- ShuffleQueryStage (19), Statistics(sizeInBytes=1031.3 KiB, rowCount=6.00E+3)\n",
    "         +- Exchange (18)\n",
    "            +- MergeRows (17)\n",
    "               +- * SortMergeJoin RightOuter (16)\n",
    "                  :- * Sort (8)\n",
    "                  :  +- AQEShuffleRead (7)\n",
    "                  :     +- ShuffleQueryStage (6), Statistics(sizeInBytes=7.8 MiB, rowCount=3.00E+4)\n",
    "                  :        +- Exchange (5)\n",
    "                  :           +- * Filter (4)\n",
    "                  :              +- * Project (3)\n",
    "                  :                 +- * ColumnarToRow (2)\n",
    "                  :                    +- BatchScan local.db.emp_bv_details (1)\n",
    "                  +- * Sort (15)\n",
    "                     +- AQEShuffleRead (14)\n",
    "                        +- ShuffleQueryStage (13), Statistics(sizeInBytes=140.7 KiB, rowCount=3.00E+3)\n",
    "                           +- Exchange (12)\n",
    "                              +- * Project (11)\n",
    "                                 +- * ColumnarToRow (10)\n",
    "                                    +- BatchScan local.db.emp_bv_updates (9)\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- No filters pushed down during Batch Scan\n",
    "(1) BatchScan local.db.emp_bv_details\n",
    "Output [7]: [id#498L, year#499, business_vertical#500, _file#507, _pos#508L, _spec_id#505, _partition#506]\n",
    "local.db.emp_bv_details (branch=null) [filters=, groupedBy=]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Merge Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TGT_TBL = \"local.db.emp_bv_details\"\n",
    "STG_TBL = \"local.db.emp_bv_updates\"\n",
    "\n",
    "# To avoid Pre-Sorting before writing into table\n",
    "spark.sql(f\"\"\"ALTER TABLE {TGT_TBL} SET TBLPROPERTIES (\n",
    "    'write.spark.fanout.enabled'='true'\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid sort due to Sort Merge Join by prefering Hash Join if possible.\n",
    "spark.conf.set('spark.sql.join.preferSortMergeJoin', 'false')\n",
    "\n",
    "# To avoid Shuffle before writing into table.\n",
    "spark.conf.set(\"spark.sql.iceberg.distribution-mode\", \"none\")\n",
    "\n",
    "# Enabling SPJ\n",
    "spark.conf.set('spark.sql.sources.v2.bucketing.enabled','true')\n",
    "spark.conf.set('spark.sql.sources.v2.bucketing.pushPartValues.enabled','true')\n",
    "spark.conf.set('spark.sql.iceberg.planning.preserve-data-grouping','true')\n",
    "spark.conf.set('spark.sql.requireAllClusterKeysForCoPartition','false')\n",
    "spark.conf.set('spark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled','true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating some employees departments after setting some configs for optimization\n",
    "updated_records = spark.range(28000, 35001).withColumn(\"year\", lit(2025)).withColumn(\"business_vertical\", lit(\"DataEngineering\"))\n",
    "updated_records.coalesce(1).writeTo(STG_TBL).overwritePartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+-----+\n",
      "|year|business_vertical|count|\n",
      "+----+-----------------+-----+\n",
      "|2025|  DataEngineering| 7001|\n",
      "+----+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(STG_TBL).groupBy(\"year\", \"business_vertical\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important things to notice:\n",
    "# 1. filter applied in on condition -- any filter here will be pushed down.\n",
    "# 2. partitioned column in join, if possible to enabled SPJ\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {TGT_TBL} as tgt\n",
    "USING (SELECT *, False as is_updated from {STG_TBL}) as src\n",
    "ON tgt.id = src.id AND tgt.year = src.year AND tgt.year = 2025\n",
    "WHEN MATCHED AND \n",
    "    tgt.business_vertical <> src.business_vertical \n",
    "    THEN\n",
    "    UPDATE SET tgt.is_updated = True, tgt.business_vertical = src.business_vertical\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Plan AFTER Optimization\n",
    "```sql\n",
    "== Physical Plan ==\n",
    "WriteDelta (16)\n",
    "+- AdaptiveSparkPlan (15)\n",
    "   +- == Final Plan ==\n",
    "      MergeRows (9)\n",
    "      +- * ShuffledHashJoin RightOuter BuildRight (8)\n",
    "         :- * Filter (4)\n",
    "         :  +- * Project (3)\n",
    "         :     +- * ColumnarToRow (2)\n",
    "         :        +- BatchScan local.db.emp_bv_details (1)\n",
    "         +- * Project (7)\n",
    "            +- * ColumnarToRow (6)\n",
    "               +- BatchScan local.db.emp_bv_updates (5)\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- Push down filter of year=2025\n",
    "(1) BatchScan local.db.emp_bv_details\n",
    "Output [7]: [id#770L, year#771, business_vertical#772, _file#779, _pos#780L, _spec_id#777, _partition#778]\n",
    "local.db.emp_bv_details (branch=null) [filters=year IS NOT NULL, year = 2025, groupedBy=year]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(f\"{TGT_TBL}.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+\n",
      "|is_updated|min(id)|max(id)|\n",
      "+----------+-------+-------+\n",
      "|      true|  28000|  29999|\n",
      "|     false|  30000|  35000|\n",
      "+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min\n",
    "spark.table(TGT_TBL).where(\"business_vertical = 'DataEngineering'\").groupBy(\"is_updated\").agg(min(\"id\"), max(\"id\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
